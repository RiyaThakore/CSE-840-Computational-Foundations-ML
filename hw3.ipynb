{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpTzZ7DXRDnW"
      },
      "source": [
        "# Question 5 (Homework 3)\n",
        "\n",
        "For this question, you will write an implementation of the logistic regression model with maximum likelihood approach using the gradient descent algorithm. The dataset for this question corresponds to the autism screening data created by Dr Fadi Fayez Thabtah using a mobile app called ASDTests (ASDtests.com). Your task is to train a logistic regression model to classify the toddlers based on their various predictor attributes. You will need to train your model on a given training set and report its classification performance on a separate test set.\n",
        "\n",
        "The code must be written using only the built-in functions in standard python as well as functions provided by the numpy and pandas libraries. You may also use the train_test_split() function in scikit-learn for this homework. All other functions in the scikit-learn library or other libraries are prohibited. If you're unsure whether you can use other functions/libraries, please contact the instructor. \n",
        "\n",
        "Follow the step-by-step procedure given in this tempate. Rename the template file to yourlastname.ipynb and submit the Jupyter notebook along with its HTML version (by choosing File -> Download as HTML on the menu option). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3KMwUoxRDnZ"
      },
      "source": [
        "**Step 1: Import Data**\n",
        "\n",
        "First, you need to download the autism data from D2L. Load the input data into a pandas DataFrame object and display it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1do8fvgRDnZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "ce465e37-725c-408e-e61f-ee27a076661d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Case_No  A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  Age_Months  \\\n",
              "0           1   0   0   0   0   0   0   1   1   0    1          28   \n",
              "1           2   1   1   0   0   0   1   1   0   0    0          36   \n",
              "2           3   1   0   0   0   0   0   1   1   0    1          36   \n",
              "3           4   1   1   1   1   1   1   1   1   1    1          24   \n",
              "4           5   1   1   0   1   1   1   1   1   1    1          20   \n",
              "...       ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...         ...   \n",
              "1049     1050   0   0   0   0   0   0   0   0   0    1          24   \n",
              "1050     1051   0   0   1   1   1   0   1   0   1    0          12   \n",
              "1051     1052   1   0   1   1   1   1   1   1   1    1          18   \n",
              "1052     1053   1   0   0   0   0   0   0   1   0    1          19   \n",
              "1053     1054   1   1   0   0   1   1   0   1   1    0          24   \n",
              "\n",
              "      Qchat-10-Score  Sex  Ethnicity  Jaundice  Family_mem_with_ASD  Class  \n",
              "0                  3    1          0         1                    0      0  \n",
              "1                  4    0          1         1                    0      1  \n",
              "2                  4    0          0         1                    0      1  \n",
              "3                 10    0          0         0                    0      1  \n",
              "4                  9    1          1         0                    1      1  \n",
              "...              ...  ...        ...       ...                  ...    ...  \n",
              "1049               1    1          1         0                    1      0  \n",
              "1050               5    0          0         1                    0      1  \n",
              "1051               9    0          0         1                    0      1  \n",
              "1052               3    0          1         0                    1      0  \n",
              "1053               6    0          0         1                    1      1  \n",
              "\n",
              "[1054 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e0a312d5-5ba7-41c1-868f-4da7e12a3218\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Case_No</th>\n",
              "      <th>A1</th>\n",
              "      <th>A2</th>\n",
              "      <th>A3</th>\n",
              "      <th>A4</th>\n",
              "      <th>A5</th>\n",
              "      <th>A6</th>\n",
              "      <th>A7</th>\n",
              "      <th>A8</th>\n",
              "      <th>A9</th>\n",
              "      <th>A10</th>\n",
              "      <th>Age_Months</th>\n",
              "      <th>Qchat-10-Score</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>Jaundice</th>\n",
              "      <th>Family_mem_with_ASD</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1049</th>\n",
              "      <td>1050</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1050</th>\n",
              "      <td>1051</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1051</th>\n",
              "      <td>1052</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1052</th>\n",
              "      <td>1053</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1053</th>\n",
              "      <td>1054</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1054 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0a312d5-5ba7-41c1-868f-4da7e12a3218')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e0a312d5-5ba7-41c1-868f-4da7e12a3218 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e0a312d5-5ba7-41c1-868f-4da7e12a3218');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Load the input data into a pandas DataFrame object \n",
        "data = pd.read_csv('/content/sample_data/autism.csv')  \n",
        "\n",
        "#Display\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh7_9GlVRDna"
      },
      "source": [
        "**Step 2: Data preprocessing** \n",
        "\n",
        "You need to perform the following steps to extract the predictor attributes X and target attribute y from the input data.\n",
        "- Extract y from the last column of the data frame. Convert y from a pandas Series object into a numpy array object.\n",
        "- Extract X from the remaining columns of the data frame. \n",
        "- Drop the column named Case_No\n",
        "- Normalize the values in the Age_Months and Qchat-10-Score columns so that their values range between 0 and 1 by dividing the values in the columns with their respective maximum value.\n",
        "- Add a column of 1s to X (this will allow us to define the model intercept/bias).\n",
        "- Convert X from a pandas DataFrame object into a numpy array object.\n",
        "\n",
        "*Hint:* You can use .values to convert a pandas Series or DataFrame object into numpy arrays. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nev9qkwoRDna"
      },
      "source": [
        "**(a)** Extract the target (class) attribute y from the last column of the dataframe. Show the class distribution. Convert y from a pandas Series object into a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# extract y & to arrary\n",
        "y = data['Class'].values\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOrRHfWTHRQP",
        "outputId": "ec0c6778-778d-4adc-a709-5b23abb44e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1054,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTuAIZhWRDnb",
        "outputId": "05d2dd45-2ff5-4b3d-e857-c804cf4bfff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       0\n",
            "1       1\n",
            "2       1\n",
            "3       1\n",
            "4       1\n",
            "       ..\n",
            "1049    0\n",
            "1050    1\n",
            "1051    1\n",
            "1052    0\n",
            "1053    1\n",
            "Name: Class, Length: 1054, dtype: int64\n",
            "Class distribution:\n",
            "1    0.690702\n",
            "0    0.309298\n",
            "Name: Class, dtype: float64\n",
            "<class 'numpy.ndarray'>\n",
            "[[0. 0. 0. ... 1. 0. 0.]\n",
            " [1. 1. 0. ... 1. 0. 1.]\n",
            " [1. 0. 0. ... 1. 0. 1.]\n",
            " ...\n",
            " [1. 0. 1. ... 1. 0. 1.]\n",
            " [1. 0. 0. ... 0. 1. 0.]\n",
            " [1. 1. 0. ... 1. 1. 1.]]\n",
            "(1054, 17)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Extract y from the last column of the data frame.\n",
        "y = data.Class\n",
        "\n",
        "print(y)\n",
        "#Class distribution\n",
        "print('Class distribution:')\n",
        "print(y.value_counts()/len(y))\n",
        "\n",
        "\n",
        "#Convert y from a pandas Series object into a numpy array object.\n",
        "y = data.values\n",
        "print(type(y))\n",
        "\n",
        "print(y)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nscz0SztRDnb"
      },
      "source": [
        "**(b)** Extract the predictor attributes X by drop the class and Case_No columns from the dataframe object. Normalize the Age_Months and Qchat-10-Score columns by dividing each value with its respective maximum value. Convert the resulting pandas dataframe object into a numpy array. Print the size of the 2d array and display X to verify its content. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = data.iloc[:,0:17] #extract class\n",
        "print(X)\n",
        "\n",
        "#Extract X from the remaining columns of the data frame and drop the column named Case_No\n",
        "X = data.drop(['Case_No'], axis=1, inplace=True)\n",
        "\n",
        "#Normalize the values in the Age_Months and Qchat-10-Score columns by dividing the values with maximum value.\n",
        "data.Age_Months = data.Age_Months / data.Age_Months.max()\n",
        "#print(data.head())\n",
        "\n",
        "data = data.rename(columns={'Qchat-10-Score': 'Quant10Score'})\n",
        "data.Quant10Score = data.Quant10Score / data.Quant10Score.max()\n",
        "#print(data.head())\n",
        "\n",
        "#Add a column of 1s to X\n",
        "#np.hstack((X,np.ones([X.shape[0],1], X.dtype)))\n",
        "\n",
        "#Convert X from a pandas DataFrame object into a numpy array object.\n",
        "X = data.values\n",
        "print(type(X))\n",
        "\n",
        "\n",
        "print(X)\n",
        "\n",
        "print('Shape of 2d-array, X =', X.shape)\n",
        "\n",
        "\n",
        "X\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nP5r6LWG16B",
        "outputId": "54ec9d5f-696f-4b09-9dba-124cc4323ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Case_No  A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  Age_Months  \\\n",
            "0           1   0   0   0   0   0   0   1   1   0    1          28   \n",
            "1           2   1   1   0   0   0   1   1   0   0    0          36   \n",
            "2           3   1   0   0   0   0   0   1   1   0    1          36   \n",
            "3           4   1   1   1   1   1   1   1   1   1    1          24   \n",
            "4           5   1   1   0   1   1   1   1   1   1    1          20   \n",
            "...       ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...         ...   \n",
            "1049     1050   0   0   0   0   0   0   0   0   0    1          24   \n",
            "1050     1051   0   0   1   1   1   0   1   0   1    0          12   \n",
            "1051     1052   1   0   1   1   1   1   1   1   1    1          18   \n",
            "1052     1053   1   0   0   0   0   0   0   1   0    1          19   \n",
            "1053     1054   1   1   0   0   1   1   0   1   1    0          24   \n",
            "\n",
            "      Qchat-10-Score  Sex  Ethnicity  Jaundice  Family_mem_with_ASD  \n",
            "0                  3    1          0         1                    0  \n",
            "1                  4    0          1         1                    0  \n",
            "2                  4    0          0         1                    0  \n",
            "3                 10    0          0         0                    0  \n",
            "4                  9    1          1         0                    1  \n",
            "...              ...  ...        ...       ...                  ...  \n",
            "1049               1    1          1         0                    1  \n",
            "1050               5    0          0         1                    0  \n",
            "1051               9    0          0         1                    0  \n",
            "1052               3    0          1         0                    1  \n",
            "1053               6    0          0         1                    1  \n",
            "\n",
            "[1054 rows x 17 columns]\n",
            "<class 'numpy.ndarray'>\n",
            "[[0. 0. 0. ... 1. 0. 0.]\n",
            " [1. 1. 0. ... 1. 0. 1.]\n",
            " [1. 0. 0. ... 1. 0. 1.]\n",
            " ...\n",
            " [1. 0. 1. ... 1. 0. 1.]\n",
            " [1. 0. 0. ... 0. 1. 0.]\n",
            " [1. 1. 0. ... 1. 1. 1.]]\n",
            "Shape of 2d-array, X = (1054, 17)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 1., 0., 0.],\n",
              "       [1., 1., 0., ..., 1., 0., 1.],\n",
              "       [1., 0., 0., ..., 1., 0., 1.],\n",
              "       ...,\n",
              "       [1., 0., 1., ..., 1., 0., 1.],\n",
              "       [1., 0., 0., ..., 0., 1., 0.],\n",
              "       [1., 1., 0., ..., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b763dnYPRDnc"
      },
      "source": [
        "**Step 3: Training and Test Set Creation**\n",
        "\n",
        "Divide the data into a separate training and test set. You may use the scikit-learn train_test_split function to do this. You need to reserve 67% of the data for training and the remaining 33% for testing. Make sure you set the random_state parameter to 1 to ensure reproducibility of your result."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAfW93TMHDn3",
        "outputId": "6eca1fee-4f76-4349-ecee-5830ff9e6e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1054, 17) (1054,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#np.hstack((X,np.ones([X.shape[0],1], X.dtype)))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=1)\n",
        "\n",
        "print('Training set: X.shape = ', X_train.shape, ' y.shape = ', y_train.shape )\n",
        "print('Test set: X.shape = ', X_test.shape, ' y.shape = ', y_test.shape )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmCVaj3TGYRr",
        "outputId": "e7e3d130-b705-4ea8-d2da-735b29917d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: X.shape =  (702, 17)  y.shape =  (702,)\n",
            "Test set: X.shape =  (352, 17)  y.shape =  (352,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8t6G5tARDnc"
      },
      "source": [
        "**Step 4: Logistic Regression Model**\n",
        "\n",
        "Write an implementation of a class named LogisticRegr that implements the logistic regression classifier. There are 2 functions that must be implemented in this class: (1) a fit() function to fit the classifier to the given input data using gradient descent algorithm, and (2) a predict() function to apply the classifier to the test data. \n",
        "\n",
        "During training, the logistic regression classifier should implement the gradient descent algorithm by minimizing the following negative loglikehood function:\n",
        "$$\\mathcal{L} = \\sum_{i=1}^N \\bigg[ y_i \\log\\bigg(1 + e^{-\\mathbf{w}^T \\mathbf{x}_i - w_0}\\bigg )  + (1 - y_i) \\log\\bigg(1 + e^{\\mathbf{w}^T\\mathbf{x}_i + w_0}\\bigg) \\bigg]$$\n",
        "where $N$ is the size of the training data, $w_0$ is the model intercept, and $\\mathbf{w}$ is the vector of model coefficients. Both the model intercept and coefficients must be initialized to 0 and vector of zeros, respectively, before performing the gradient descent. You must compute the training loss at every iteration and store them in an array named loss. The training loss will be returned by the fit() function, which you can use to plot the convergence of the algorithm.\n",
        "\n",
        "Create a class named LogisticRegr for the logistic regression model. The model contains 2 functions:\n",
        "- fit(): this function will estimate the parameters of the generalized linear model using the maximum likelihood approach with gradient descent algorithm. The pseudocode of the gradient descent algorithm is as follows:\n",
        "\n",
        "    - w = [0,0,...,0]\n",
        "    - for i = 1 to maxiter do\n",
        "        - Update the weight: $\\mathbf{w} \\leftarrow \\mathbf{w} - \\frac{\\alpha}{N} \\nabla \\mathcal{L}$ (where $\\alpha$ is the learning rate) \n",
        "    - end\n",
        "\n",
        "- predict(): this function will apply the generalized linear model to predict the values of the target attribute given some test data, where the predicted class probability for each test instance $\\mathbf{x}_i$ is $$P(y_{i, pred} = 1 | \\mathbf{x}_i) = \\sigma(\\mathbf{x}_i) = \\frac{1}{1 + \\exp[-\\mathbf{w}^T\\mathbf{x}_i - w_0]}$$\n",
        "The function will assign the test instance to the larger class, e.g., class 1 if $P(y_{i,pred} = 1 | \\mathbf{x}_i) > P(y_{i,pred} = 0 | \\mathbf{x}_i)$. The function should return both the predicted class as well as the predicted class probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW3QzBhrRDnc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class LogisticRegr():\n",
        "    \"\"\"\n",
        "    Implementation of logistic regression classifier.\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.w = np.zeros(17)\n",
        "\n",
        "    def fit(self, X, y, maxiter = 100, learning_rate = 0.01):\n",
        "        \"\"\"\"\n",
        "            Input:  \n",
        "                X: N x d matrix of predictor attributes, where N is #training points and d is #predictor attributes\n",
        "                y: N x 1 vector of target attributes, where each value in the vector is either 0 or 1.\n",
        "                maxiter: maximum iteration before the gradient/subgradient descent algorithm terminates (default=100).\n",
        "                learning_rate: learning rate for gradient/subgradient descent (default = 0.01).\n",
        "\n",
        "            Output: \n",
        "                loss: maxiter x 1 vector containing value of the loss function in each iteration\n",
        "        \"\"\"    \n",
        "        n = len(X)\n",
        "        loss = []\n",
        "        wt = self.wt\n",
        "\n",
        "        for i in range(1, maxiter):\n",
        "            d, l = 0,0\n",
        "            for j in range(0, n):\n",
        "                sigma = 1/(1 + math.e**(-np.dot(wt.T,X[j])))\n",
        "                N = -((y[j] / sigma) - ((1 - y[j])/(1 - sigma)))\n",
        "                d += N * (sigma * (1 - sigma) * X[j])\n",
        "                l = l + y[j] * math.log(1 + math.e**(-np.dot(wt.T,X[j]))) + (1 - y[j]) * math.log(1 + math.e**(np.dot(wt.T,X[j])))\n",
        "            loss.append(l)\n",
        "            wt = wt - (learning_rate / n) * d\n",
        "        self.wt = wt\n",
        "\n",
        "        return loss\n",
        "                \n",
        "    def predict(self, X):\n",
        "        \"\"\"\"\n",
        "            Input:  \n",
        "                X: N x d matrix of predictor attributes, where N is #data points and d is #predictor attributes\n",
        "\n",
        "            Output: \n",
        "                Y_pred: N x 1 vector containing the predicted class of each data point (either 0 or 1)\n",
        "                Y_probs: N x 2 vector containing posterior probabilities of each data point in each of the 2 classes,\n",
        "                         where Y_probs[:,0] = P(y=0|x) and Y_probs[:,1] = P(y=1|x). \n",
        "        \"\"\"    \n",
        "        n = len(X)\n",
        "        Y_pred = []\n",
        "        Y_probs = [[],[]]\n",
        "        w = self.w\n",
        "\n",
        "        for k in range(0,n):\n",
        "            a = 1/(1 + math.e**(-np.dot(w.T,X[k])))\n",
        "            b = 1 - a\n",
        "            Y_probs[0].append(a)\n",
        "            Y_probs[1].append(b)\n",
        "            if (b > a):\n",
        "                Y_pred.append(1)\n",
        "            else:\n",
        "                Y_pred.append(0)\n",
        "\n",
        "        return (Y_pred, Y_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq1QKPeJRDnd"
      },
      "source": [
        "**(a)** Train the logistic regression on the training data by invoking the fit() function with maxiter = 1000. Plot the resulting loss for each iteration to show convergence of the gradient descent algorithm. Make sure you display the model coefficients, including its intercept term."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "model = LogisticRegr()\n",
        "losses = model.fit(X_train, y_train, maxiter=1000)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title('Logistic regression')\n",
        "print('Model coefficients:', model.w[1:])\n",
        "print('Model intercept:', model.w[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "k61Oe92ngCkP",
        "outputId": "4753d6cd-ecf2-49de-edf6-7398aee9903c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model coefficients: [ 0.3612804   0.20266443  0.31905505  0.34997182  0.34912834  0.31314045\n",
            "  0.24677765  0.45043439 -0.12771301 -0.53079407  0.27591738 -0.37254931\n",
            " -0.10771429 -0.07328131 -0.11162494  0.9551069 ]\n",
            "Model intercept: 0.2944342679924085\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMpBucZNRDnd"
      },
      "source": [
        "**(b)** Apply the trained model separately to both the training and test data. Report the accuracy and confusion matrix obtained. You may use scikit-learn library functions for confusion_matrix and accuracy_score for this step."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "model = LogisticRegr()\n",
        "losses = model.fit(X_train, y_train, maxiter=1000)\n",
        "\n",
        "(ytrain_pred, probs) = model.predict(X_train)\n",
        "(ytest_pred, probs) = model.predict(X_test)\n",
        "\n",
        "print('Model performance on training set:')\n",
        "print('Accuracy:', accuracy_score(y_train, ytrain_pred))\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(y_train, ytrain_pred))\n",
        "\n",
        "print('\\nModel performance on test set:')\n",
        "print('Accuracy:', accuracy_score(y_test, ytest_pred))\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(y_test, ytest_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp_VXPGhHu-T",
        "outputId": "75aa9b56-f51b-4632-916e-9f4a412891b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance on training set:\n",
            "Accuracy: 0.8931623931623932\n",
            "Confusion matrix:\n",
            "[[157  75]\n",
            " [  0 470]]\n",
            "\n",
            "Model performance on test set:\n",
            "Accuracy: 0.9204545454545454\n",
            "Confusion matrix:\n",
            "[[ 66  28]\n",
            " [  0 258]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}